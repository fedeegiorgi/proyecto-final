% called by main.tex
%
\chapter{Discusión}
\label{ch::capitulo8}

En esta sección, nos proponemos interpretar los resultados obtenidos y expuestos anteriormente. Al mismo tiempo, se buscará analizar los hallazgos y discutir las posibles causas de lo observado en concordancia con el trabajo de experimentación llevado a cabo.

En primer lugar, el resultado más relevante es el rendimiento observado de las variantes propuestas de RF que incorporan la etapa intermedia de debate en comparación al algoritmo clásico. Como se puede ver una vez ejecutado el test de hipótesis no paramétrico Kruskal-Wallis sobre los diferentes modelos, existen diferencias significativas entre los mismos para cinco de los seis conjuntos de entrenamiento evaluados. Esto se deriva de que el p-valor obtenido fue menor a $0.05$, con excepción del dataset \textit{Wind} (ver \hyperref[tab1]{Tabla 1}).

Sin embargo, con el análisis post-hoc utilizando el test de Dunn, se puede observar entre las comparaciones modelo a modelo, que el único modelo que presenta diferencias significativas con el resto es \texttt{FirstSplitCombinerRandomForestRegressor}. Esto mismo se puede notar dado que la matriz resultante del análisis de Dunn, muestra unos para todas las columnas y filas, a excepción de la fila y columna correspondiente al modelo de combinación árboles de primeros cortes. A su vez, se puede verificar lo mismo observando el orden para los distintos datasets entre los modelos para cada fold del cross-validation llevado a cabo. Para muchos de los folds, el modelo en cuestión se encuentra al final de la tabla indicando que su performance en comparación de los otros modelos fue inferior. También se puede constatar esto mismo en la \hyperref[figure11]{Figura 11}, dónde se ve la mediana para cada modelo entre los distintos folds, donde el modelo de combinación de árboles tiene una mediana efectivamente superior de MSE que el resto de los modelos y, a su vez, entre los otros modelos no se notan prácticamente diferencias visualmente.

Con esa información, entonces, podemos confirmar que el modelo de combinación de árboles de primeros cortes tiene efectivamente un rendimiento inferior en comparación tanto del modelo original como de las otras alternativas propuestas. Por otro lado, al quitar al modelo de combinación del análisis y evaluar nuevamente con el test no paramétrico vemos que claramente los p-valores son mucho más grandes que $0.05$ por lo que no hay evidencia suficiente para rechazar la hipótesis nula del test. Esto demuestra que las variaciones que incluyen la simulación del debate entre árboles exploradas en este trabajo no mostraron mejoras significativas con respecto al algoritmo de RF clásico, contrario a la hipótesis planteada.

Si bien no pudo ser confirmada la hipótesis de que un algoritmo de ensamble como RF mejore su rendimiento con mecanismos de agregación entre predicciones diferentes, tampoco se puede descartar por completo la hipótesis. A lo largo de este trabajo, si bien se exploraron numerosas alternativas de simulación de etapa intermedia de deliberación, podrían haber mecanismos no explorados que imiten de manera más eficiente lo observado con personas en el experimento de psicología experimental llevado a cabo por Navajas.

Además, observando con detenimiento las tablas con el orden de resultados, se puede notar que para algunos conjuntos de instancias de validación (folds), algunas de las variantes propuestas en esta investigación superaron al algoritmo original. Esto puede motivar a pensar que futuros ajustes y refinamientos ya sea de las implementaciones u optimización de hiperparámetros no explorados permitan una mejora significativa.

Enfocándonos ahora en los motivos de estos resultados, existen diversas interpretaciones e ideas que se pueden plantear. Con respecto a por qué el modelo de combinación de árboles de primeros cortes (FSC) tuvo un rendimiento por lejos inferior que el resto, consideramos que el principal motivo se encuentra en la naturaleza de la combinación misma de árboles. Recordando esta implementación, para combinar los árboles se toma únicamente el primer corte o división de los mismos. Observando algunas instancias de evaluación se puede notar que, si bien controlado por el hiperparámetro \texttt{max\_features} no todos los árboles tuvieron a disposición las mismas features, cada árbol que tenga la característica óptima de corte la utilizará para su primer división. De esta forma, para cada grupo de árboles, puede suceder que varios árboles tengan como feature de primer corte a la misma variable con pequeñas variaciones en el valor del umbral de división. Eso provocará que a lo largo de los niveles del árbol combinado se repita la misma variable de corte haciendo que el árbol tenga sucesivos cortes que no aporten información y sean redundantes. Es así como, muy probablemente cada árbol combinado perdió información valiosa de los árboles iniciales provocando que el modelo pierda capacidad predictiva.

Por otro lado, el hecho de que para el dataset \textit{Wind}, no haya diferencias significativas con \texttt{FirstSplitCombinerRandomForestRegressor} podría ser causa de particularidades específicas del conjunto de datos en cuestión. En este caso, es un conjunto de datos con diez variables numéricas. Una posible explicación es que al ser relativamente no muchas features, el hiperparámetro \texttt{max\_features} forzó a que los árboles usen como primer corte diversas features, provocando que el problema de redundancia tenga un impacto mucho menor que para los otros conjuntos de entrenamiento. Esto indicaría que el modelo de combinación de árboles de primeros cortes, en algunas circunstancias, no tendría una performance significativamente inferior al resto. Sin embargo, dado que esto sólo ocurre en uno de los seis datasets, no modifica sustancialmente las conclusiones descritas anteriormente.

Otra observación a notar de los resultados es la relación entre la performance de los modelos y el cumplimiento de la distribución de Navajas. En los conjuntos de entrenamiento cuyas predicciones no siguen la distribución log-normal, no se evidencia una diferencia significativa en la performance en comparación con RF. Esto podría indicar que para las alternativas exploradas, la distribución no es un factor determinante para la performance de los modelos. Sin embargo, hay un pequeño detalle que se puede notar observando con detenimiento la tabla de ranking de \textit{cross-validation} (\hyperref[appendix8]{Apéndice 8}). Para aquellos conjuntos de datos con los que las predicciones de los árboles no siguen una distribución log-normal, se nota que los modelos \texttt{OOBRandomForestRegressor} y \texttt{OOBPlusIQRRandomForestRegressor} presentan resultados idénticos. Esto muy posiblemente se deba a que entre las predicciones de los árboles en cada grupo no hay \textit{outliers} claros por lo que el mecanismo de exclusión en \texttt{OOBPlusIQRRandomForestRegressor} no tiene ningún impacto. Además, estos modelos tienen como hiperparámetros \texttt{n\_estimators} y \texttt{group\_size} los mismos valores. Para verificar esta hipótesis, realizamos una ejecución de los modelos en cuestión sumado a \texttt{IQRRandomForestRegressor} y RF original. Se puede notar en la tabla del \hyperref[appendix9]{Apéndice 9} que, ante los mismos valores de \texttt{n\_estimators} y \texttt{max\_depth}, entre \texttt{OOBRandomForestRegressor} y \texttt{OOBPlusIQRRandomForestRegressor} por un lado y entre \texttt{IQRRandomForestRegressor} y RF original, los MSE que arrojan coinciden. Esto sustenta la teoría que la exclusión por el método de IQR no sirve para datasets que no cumplen las condiciones descriptas previamente.

Finalmente, en cuanto a las razones por la que no se notaron diferencias significativas entre las demás variantes y RF original, no parecen haber motivos o causas que se puedan identificar de manera sencilla. Un posible motivo, el cual consideramos bastante probable, es que si bien nuestras alternativas son diversas conceptualmente entre si y con el algoritmo original, todas tienen un fuerte foco en la etapa de agregación de información entre los árboles individuales y no alteran sustancialmente la fase de entrenamiento de los mismos. 

Otros modelos de ensamble que han tomado ideas similares a Random Forest y han logrado superarlo, como puede ser \textit{XGBoost}, trabajan de lleno con la construcción inicial de los árboles. Este enfoque puede tener un mayor peso sobre la capacidad predictiva del modelo que la manera de ensamblar en sí, siempre y cuando esta sea razonable. Los malos resultados obtenidos en FSC podrían acercarnos a esta posible explicación, dado que su manera de combinar los árboles es la mas alejada de la del algoritmo original, mientras que los otros algoritmos de alguna forma u otra terminan realizando también un promedio.
