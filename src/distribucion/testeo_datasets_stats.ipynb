{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.cm import get_cmap\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.io import arff\n",
    "import os\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import lognorm\n",
    "from scipy.stats import norm\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "\n",
    "SEED = 14208"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_filepath = 'datasets/wind_dataset_transformed.csv'  # Aquí se incluirían las rutas a los datasets\n",
    "pred_col_name = 'WIND'  # Columna a predecir\n",
    "alpha = 0.1  # Nivel de significancia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(dataset_filepath: str, pred_col_name: str):\n",
    "    # Obtener la extension del archivo\n",
    "    _, file_extension = os.path.splitext(dataset_filepath)\n",
    "\n",
    "    # Cargar el dataset según la extensión\n",
    "    if file_extension == '.arff':\n",
    "        data = arff.loadarff(dataset_filepath)\n",
    "        df = pd.DataFrame(data[0])\n",
    "\n",
    "    elif file_extension == '.csv':\n",
    "        df = pd.read_csv(dataset_filepath)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Formato no soportado, `dataset_filepath` debe tener una de las siguientes extensiones: .csv, .arff\")\n",
    "\n",
    "    # Separar el dataset en Train y Validation\n",
    "    train_df, validation_df = train_test_split(df, test_size=0.2, random_state=SEED)\n",
    "    y_train, y_valid = train_df[pred_col_name], validation_df[pred_col_name]\n",
    "    X_train, X_valid = train_df.drop(pred_col_name, axis=1), validation_df.drop(pred_col_name, axis=1)\n",
    "\n",
    "    # Aplicar get_dummies para variables categóricas\n",
    "    X_train = pd.get_dummies(X_train)\n",
    "    X_valid = pd.get_dummies(X_valid)\n",
    "    X_train, X_valid = X_train.align(X_valid, join='left', axis=1, fill_value=0) \n",
    "\n",
    "    # Crear y entrenar el modelo de Random Forest\n",
    "    rf_model = RandomForestRegressor(random_state=SEED)\n",
    "    rf_model.fit(X_train, y_train)\n",
    "\n",
    "    # Obtener las predicciones de cada árbol en el bosque\n",
    "    tree_predictions = []\n",
    "    for tree in rf_model.estimators_:\n",
    "        tree_pred = tree.predict(X_valid)\n",
    "        tree_predictions.append(tree_pred)\n",
    "\n",
    "    tree_predictions = np.array(tree_predictions).T\n",
    "\n",
    "    # Evaluar el modelo\n",
    "    predictions = rf_model.predict(X_valid)\n",
    "    mse = mean_squared_error(y_valid, predictions)\n",
    "\n",
    "    return df, mse, predictions, tree_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lognormal_KS_fit_check(tree_predictions, shift=1e-6, alpha=0.05, orig_data_stat=0.0722):\n",
    "    ks_results = []\n",
    "    tree_predictions += shift\n",
    "    for predictions in tree_predictions:\n",
    "        # Estimate parameters of the lognormal distribution\n",
    "        shape, loc, scale = stats.lognorm.fit(predictions, floc=0)\n",
    "        \n",
    "        # Perform the Kolmogorov-Smirnov test\n",
    "        ks_stat, p_value = stats.kstest(predictions, 'lognorm', args=(shape, loc, scale))\n",
    "        ks_results.append((ks_stat, p_value, shape, loc, scale))\n",
    "\n",
    "    # Proportion of trees that fit the lognormal distribution\n",
    "    ks_stats = [result[0] for result in ks_results]\n",
    "    eps = alpha * orig_data_stat\n",
    "    filtered_stats = [stat for stat in ks_stats if orig_data_stat-eps <= stat <= orig_data_stat+eps]\n",
    "    ks_fit_proportion = len(filtered_stats) / len(ks_stats)\n",
    "    \n",
    "    return ks_fit_proportion, ks_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gamma_KS_fit_check(tree_predictions, shift=1e-6, alpha=0.05, orig_data_stat=0.1323):\n",
    "    ks_results = []\n",
    "    tree_predictions += shift\n",
    "    for predictions in tree_predictions:\n",
    "        # Estimate parameters of the lognormal distribution\n",
    "        shape, loc, scale = stats.gamma.fit(predictions, floc=0)\n",
    "        \n",
    "        # Perform the Kolmogorov-Smirnov test\n",
    "        ks_stat, p_value = stats.kstest(predictions, 'gamma', args=(shape, loc, scale))\n",
    "        ks_results.append((ks_stat, p_value, shape, loc, scale))\n",
    "\n",
    "    # Proportion of trees that fit the lognormal distribution\n",
    "    ks_stats = [result[0] for result in ks_results]\n",
    "    eps = alpha * orig_data_stat\n",
    "    filtered_stats = [stat for stat in ks_stats if orig_data_stat-eps <= stat <= orig_data_stat+eps]\n",
    "    ks_fit_proportion = len(filtered_stats) / len(ks_stats)\n",
    "    \n",
    "    return ks_fit_proportion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transforme_normal_SW_fit_check(tree_predictions, shift=1e-6, alpha=0.05, orig_data_stat=0.9809):\n",
    "    sw_results = []\n",
    "    tree_predictions = np.log1p(tree_predictions)\n",
    "\n",
    "    for predictions in tree_predictions:\n",
    "        # Perform the Shapiro-Wilk test\n",
    "        shapiro_stat, shapiro_p_value = stats.shapiro(predictions)\n",
    "        sw_results.append((shapiro_stat, shapiro_p_value))\n",
    "\n",
    "    # Proportion of trees that fit the normal distribution\n",
    "    sw_stats = [result[0] for result in sw_results]\n",
    "    eps = alpha * orig_data_stat\n",
    "    filtered_stats = [stat for stat in sw_stats if orig_data_stat-eps <= stat <= orig_data_stat+eps]\n",
    "    sw_fit_proportion = len(filtered_stats) / len(sw_stats)\n",
    "    \n",
    "    return sw_fit_proportion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_lognormal_fit(predictions, dataset_name, save=False, shift=1e-6, given_index=0):\n",
    "#     predictions = predictions[given_index]\n",
    "\n",
    "#     plt.figure(figsize=(8, 6))\n",
    "#     sns.histplot(predictions, kde=True, stat='density', bins=30, color='green', alpha=0.6 , label='Data')\n",
    "\n",
    "#     shape, loc, scale = stats.lognorm.fit(predictions, floc=0)\n",
    "\n",
    "#     x = np.linspace(min(predictions), max(predictions), 100)\n",
    "#     pdf_lognormal = stats.lognorm.pdf(x, shape, loc, scale)\n",
    "#     plt.plot(x, pdf_lognormal, 'r-', label='Lognormal Distribution')\n",
    "\n",
    "#     plt.title(f\"Distribucion de las predicciones | Instancia {given_index} [Validation Set]\")\n",
    "#     plt.legend()\n",
    "\n",
    "#     if save:\n",
    "#         plt.savefig(f'graficos/predictions/{dataset_name}/distribution_lognormal/{i}.png', format='png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_normal_fit(predictions, dataset_name, save=False, shift=1e-6, given_index=0):\n",
    "#     predictions = predictions[given_index]\n",
    "#     predictions = np.log1p(predictions)\n",
    "\n",
    "#     plt.figure(figsize=(8, 6))\n",
    "#     sns.histplot(predictions, kde=True, stat='density', bins=30, color='green', alpha=0.6 , label='Transformed Data')\n",
    "\n",
    "#     mean, std = np.mean(predictions), np.std(predictions)\n",
    "\n",
    "#     x = np.linspace(min(predictions), max(predictions), 100)\n",
    "#     pdf_normal = stats.norm.pdf(x, mean, std)\n",
    "#     plt.plot(x, pdf_normal, 'r-', label='Normal Distribution')\n",
    "\n",
    "#     plt.title(f\"Distribucion de las predicciones | Instancia {given_index} [Validation Set]\")\n",
    "#     plt.legend()\n",
    "\n",
    "#     if save:\n",
    "#         plt.savefig(f'graficos/predictions/{dataset_name}/distribution_normal/{i}.png', format='png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of trees that fit the lognormal distribution: 0.01684\n",
      "Proportion of trees that fit the normal distribution (over transformed data): 0.83599\n"
     ]
    }
   ],
   "source": [
    "dataset, mse, predictions, tree_predictions = process_dataset(dataset_filepath, pred_col_name)\n",
    "\n",
    "ks_lognormal_fit_proportion, ks_stats = lognormal_KS_fit_check(tree_predictions, alpha=alpha)\n",
    "print(f\"Proportion of trees that fit the lognormal distribution: {ks_lognormal_fit_proportion:.5f}\")\n",
    "\n",
    "sw_fit_proportion = transforme_normal_SW_fit_check(tree_predictions, alpha=alpha)\n",
    "print(f\"Proportion of trees that fit the normal distribution (over transformed data): {sw_fit_proportion:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_name = os.path.basename(dataset_filepath)\n",
    "# file_path = 'datasets_results.csv'\n",
    "\n",
    "# # Check if the file exists\n",
    "# if os.path.exists(file_path):\n",
    "#     # Open the file\n",
    "#     df = pd.read_csv(file_path)\n",
    "# else:\n",
    "#     # Create a new dataframe with column headers\n",
    "#     df = pd.DataFrame(columns=['dataset_name', 'prop_lognormal', 'prop_normal', 'alpha', 'pred_col_name'])\n",
    "\n",
    "# # Add a new row with values\n",
    "# new_row = {'dataset_name': dataset_name, \n",
    "#            'prop_lognormal': ks_lognormal_fit_proportion, \n",
    "#            'prop_normal': sw_fit_proportion, \n",
    "#            'alpha': alpha, \n",
    "#            'pred_col_name': pred_col_name}\n",
    "# #df = df.append(new_row, ignore_index=True)\n",
    "# df = pd.concat([df, new_row], ignore_index=True)\n",
    "\n",
    "# # Save the dataframe to the file\n",
    "# df.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Extract dataset name\n",
    "dataset_name = os.path.basename(dataset_filepath)\n",
    "file_path = 'datasets_results.csv'\n",
    "\n",
    "# Check if the file exists and load or create a DataFrame\n",
    "if os.path.exists(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "else:\n",
    "    df = pd.DataFrame(columns=['dataset_name', 'prop_lognormal', 'prop_normal', 'alpha', 'pred_col_name'])\n",
    "\n",
    "# Create a new row as a DataFrame for concatenation\n",
    "new_row = pd.DataFrame([{\n",
    "    'dataset_name': dataset_name,\n",
    "    'prop_lognormal': ks_lognormal_fit_proportion,\n",
    "    'prop_normal': sw_fit_proportion,\n",
    "    'alpha': alpha,\n",
    "    'pred_col_name': pred_col_name\n",
    "}])\n",
    "\n",
    "# Concatenate the new row\n",
    "df = pd.concat([df, new_row], ignore_index=True)\n",
    "\n",
    "# Save the updated DataFrame\n",
    "df.to_csv(file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para eliminar las columnas \"columna1\" y \"columna2\"\n",
    "#df = df.drop([26, 23, 22, 20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset_name</th>\n",
       "      <th>prop_lognormal</th>\n",
       "      <th>prop_normal</th>\n",
       "      <th>alpha</th>\n",
       "      <th>pred_col_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>titanic_fare_test.arff</td>\n",
       "      <td>0.019084</td>\n",
       "      <td>0.381679</td>\n",
       "      <td>0.1</td>\n",
       "      <td>Fare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>medical_costs.csv</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078358</td>\n",
       "      <td>0.1</td>\n",
       "      <td>medical charges</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>salary_football.csv</td>\n",
       "      <td>0.002558</td>\n",
       "      <td>0.485934</td>\n",
       "      <td>0.1</td>\n",
       "      <td>Wage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Height.csv</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.417112</td>\n",
       "      <td>0.1</td>\n",
       "      <td>childHeight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wine_quality.arff</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033846</td>\n",
       "      <td>0.1</td>\n",
       "      <td>quality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>house_8L.arff</td>\n",
       "      <td>0.043889</td>\n",
       "      <td>0.695414</td>\n",
       "      <td>0.1</td>\n",
       "      <td>price</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>kidney.arff</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>frailty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>flight_price.csv</td>\n",
       "      <td>0.000468</td>\n",
       "      <td>0.106692</td>\n",
       "      <td>0.1</td>\n",
       "      <td>Price</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>boston_housing.arff</td>\n",
       "      <td>0.009804</td>\n",
       "      <td>0.598039</td>\n",
       "      <td>0.1</td>\n",
       "      <td>MEDV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>liver_disorders.arff</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.362319</td>\n",
       "      <td>0.1</td>\n",
       "      <td>drinks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>aerospace_stock.arff</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.1</td>\n",
       "      <td>company10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Concrete_Strength.csv</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.300971</td>\n",
       "      <td>0.1</td>\n",
       "      <td>Concretestrength</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>laptops.csv</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.780000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>Price</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Metro_Interstate_Traffic_Volume_test.csv</td>\n",
       "      <td>0.001258</td>\n",
       "      <td>0.222013</td>\n",
       "      <td>0.1</td>\n",
       "      <td>traffic_volume</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>used_car.csv</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.229508</td>\n",
       "      <td>0.1</td>\n",
       "      <td>Selling_Price</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>weight.csv</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.209781</td>\n",
       "      <td>0.1</td>\n",
       "      <td>weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>laptop.csv</td>\n",
       "      <td>0.005025</td>\n",
       "      <td>0.614322</td>\n",
       "      <td>0.1</td>\n",
       "      <td>Price</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>rainfall.arff</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.131603</td>\n",
       "      <td>0.1</td>\n",
       "      <td>Rainfall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>abalone.csv</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.540670</td>\n",
       "      <td>0.1</td>\n",
       "      <td>Age</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>wind_dataset.csv</td>\n",
       "      <td>0.003546</td>\n",
       "      <td>0.736702</td>\n",
       "      <td>0.1</td>\n",
       "      <td>WIND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Carbon_Emission.csv</td>\n",
       "      <td>0.058000</td>\n",
       "      <td>0.937000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>CarbonEmission</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Carbon_Emission_transformed.csv</td>\n",
       "      <td>0.085500</td>\n",
       "      <td>0.948500</td>\n",
       "      <td>0.1</td>\n",
       "      <td>CarbonEmission</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>wind_dataset_transformed.csv</td>\n",
       "      <td>0.016844</td>\n",
       "      <td>0.835993</td>\n",
       "      <td>0.1</td>\n",
       "      <td>WIND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>wind_dataset_transformed.csv</td>\n",
       "      <td>0.016844</td>\n",
       "      <td>0.835993</td>\n",
       "      <td>0.1</td>\n",
       "      <td>WIND</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                dataset_name  prop_lognormal  prop_normal  \\\n",
       "0                     titanic_fare_test.arff        0.019084     0.381679   \n",
       "1                          medical_costs.csv        0.000000     0.078358   \n",
       "2                        salary_football.csv        0.002558     0.485934   \n",
       "3                                 Height.csv        0.000000     0.417112   \n",
       "4                          wine_quality.arff        0.000000     0.033846   \n",
       "5                              house_8L.arff        0.043889     0.695414   \n",
       "6                                kidney.arff        0.000000     0.250000   \n",
       "7                           flight_price.csv        0.000468     0.106692   \n",
       "8                        boston_housing.arff        0.009804     0.598039   \n",
       "9                       liver_disorders.arff        0.000000     0.362319   \n",
       "10                      aerospace_stock.arff        0.000000     0.315789   \n",
       "11                     Concrete_Strength.csv        0.000000     0.300971   \n",
       "12                               laptops.csv        0.010000     0.780000   \n",
       "13  Metro_Interstate_Traffic_Volume_test.csv        0.001258     0.222013   \n",
       "14                              used_car.csv        0.000000     0.229508   \n",
       "15                                weight.csv        0.000000     0.209781   \n",
       "16                                laptop.csv        0.005025     0.614322   \n",
       "17                             rainfall.arff        0.000000     0.131603   \n",
       "18                               abalone.csv        0.000000     0.540670   \n",
       "19                          wind_dataset.csv        0.003546     0.736702   \n",
       "20                       Carbon_Emission.csv        0.058000     0.937000   \n",
       "21           Carbon_Emission_transformed.csv        0.085500     0.948500   \n",
       "22              wind_dataset_transformed.csv        0.016844     0.835993   \n",
       "23              wind_dataset_transformed.csv        0.016844     0.835993   \n",
       "\n",
       "    alpha     pred_col_name  \n",
       "0     0.1              Fare  \n",
       "1     0.1   medical charges  \n",
       "2     0.1              Wage  \n",
       "3     0.1       childHeight  \n",
       "4     0.1           quality  \n",
       "5     0.1             price  \n",
       "6     0.1           frailty  \n",
       "7     0.1             Price  \n",
       "8     0.1              MEDV  \n",
       "9     0.1            drinks  \n",
       "10    0.1         company10  \n",
       "11    0.1  Concretestrength  \n",
       "12    0.1             Price  \n",
       "13    0.1    traffic_volume  \n",
       "14    0.1     Selling_Price  \n",
       "15    0.1            weight  \n",
       "16    0.1             Price  \n",
       "17    0.1          Rainfall  \n",
       "18    0.1               Age  \n",
       "19    0.1              WIND  \n",
       "20    0.1    CarbonEmission  \n",
       "21    0.1    CarbonEmission  \n",
       "22    0.1              WIND  \n",
       "23    0.1              WIND  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset_name</th>\n",
       "      <th>prop_lognormal</th>\n",
       "      <th>prop_normal</th>\n",
       "      <th>alpha</th>\n",
       "      <th>pred_col_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Carbon_Emission_transformed.csv</td>\n",
       "      <td>0.085500</td>\n",
       "      <td>0.948500</td>\n",
       "      <td>0.1</td>\n",
       "      <td>CarbonEmission</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Carbon_Emission.csv</td>\n",
       "      <td>0.058000</td>\n",
       "      <td>0.937000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>CarbonEmission</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>wind_dataset_transformed.csv</td>\n",
       "      <td>0.016844</td>\n",
       "      <td>0.835993</td>\n",
       "      <td>0.1</td>\n",
       "      <td>WIND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>wind_dataset_transformed.csv</td>\n",
       "      <td>0.016844</td>\n",
       "      <td>0.835993</td>\n",
       "      <td>0.1</td>\n",
       "      <td>WIND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>laptops.csv</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.780000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>Price</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>wind_dataset.csv</td>\n",
       "      <td>0.003546</td>\n",
       "      <td>0.736702</td>\n",
       "      <td>0.1</td>\n",
       "      <td>WIND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>house_8L.arff</td>\n",
       "      <td>0.043889</td>\n",
       "      <td>0.695414</td>\n",
       "      <td>0.1</td>\n",
       "      <td>price</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>laptop.csv</td>\n",
       "      <td>0.005025</td>\n",
       "      <td>0.614322</td>\n",
       "      <td>0.1</td>\n",
       "      <td>Price</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>boston_housing.arff</td>\n",
       "      <td>0.009804</td>\n",
       "      <td>0.598039</td>\n",
       "      <td>0.1</td>\n",
       "      <td>MEDV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>abalone.csv</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.540670</td>\n",
       "      <td>0.1</td>\n",
       "      <td>Age</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>salary_football.csv</td>\n",
       "      <td>0.002558</td>\n",
       "      <td>0.485934</td>\n",
       "      <td>0.1</td>\n",
       "      <td>Wage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Height.csv</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.417112</td>\n",
       "      <td>0.1</td>\n",
       "      <td>childHeight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>titanic_fare_test.arff</td>\n",
       "      <td>0.019084</td>\n",
       "      <td>0.381679</td>\n",
       "      <td>0.1</td>\n",
       "      <td>Fare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>liver_disorders.arff</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.362319</td>\n",
       "      <td>0.1</td>\n",
       "      <td>drinks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>aerospace_stock.arff</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.1</td>\n",
       "      <td>company10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Concrete_Strength.csv</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.300971</td>\n",
       "      <td>0.1</td>\n",
       "      <td>Concretestrength</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>kidney.arff</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>frailty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>used_car.csv</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.229508</td>\n",
       "      <td>0.1</td>\n",
       "      <td>Selling_Price</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Metro_Interstate_Traffic_Volume_test.csv</td>\n",
       "      <td>0.001258</td>\n",
       "      <td>0.222013</td>\n",
       "      <td>0.1</td>\n",
       "      <td>traffic_volume</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>weight.csv</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.209781</td>\n",
       "      <td>0.1</td>\n",
       "      <td>weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>rainfall.arff</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.131603</td>\n",
       "      <td>0.1</td>\n",
       "      <td>Rainfall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>flight_price.csv</td>\n",
       "      <td>0.000468</td>\n",
       "      <td>0.106692</td>\n",
       "      <td>0.1</td>\n",
       "      <td>Price</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>medical_costs.csv</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078358</td>\n",
       "      <td>0.1</td>\n",
       "      <td>medical charges</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wine_quality.arff</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033846</td>\n",
       "      <td>0.1</td>\n",
       "      <td>quality</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                dataset_name  prop_lognormal  prop_normal  \\\n",
       "21           Carbon_Emission_transformed.csv        0.085500     0.948500   \n",
       "20                       Carbon_Emission.csv        0.058000     0.937000   \n",
       "23              wind_dataset_transformed.csv        0.016844     0.835993   \n",
       "22              wind_dataset_transformed.csv        0.016844     0.835993   \n",
       "12                               laptops.csv        0.010000     0.780000   \n",
       "19                          wind_dataset.csv        0.003546     0.736702   \n",
       "5                              house_8L.arff        0.043889     0.695414   \n",
       "16                                laptop.csv        0.005025     0.614322   \n",
       "8                        boston_housing.arff        0.009804     0.598039   \n",
       "18                               abalone.csv        0.000000     0.540670   \n",
       "2                        salary_football.csv        0.002558     0.485934   \n",
       "3                                 Height.csv        0.000000     0.417112   \n",
       "0                     titanic_fare_test.arff        0.019084     0.381679   \n",
       "9                       liver_disorders.arff        0.000000     0.362319   \n",
       "10                      aerospace_stock.arff        0.000000     0.315789   \n",
       "11                     Concrete_Strength.csv        0.000000     0.300971   \n",
       "6                                kidney.arff        0.000000     0.250000   \n",
       "14                              used_car.csv        0.000000     0.229508   \n",
       "13  Metro_Interstate_Traffic_Volume_test.csv        0.001258     0.222013   \n",
       "15                                weight.csv        0.000000     0.209781   \n",
       "17                             rainfall.arff        0.000000     0.131603   \n",
       "7                           flight_price.csv        0.000468     0.106692   \n",
       "1                          medical_costs.csv        0.000000     0.078358   \n",
       "4                          wine_quality.arff        0.000000     0.033846   \n",
       "\n",
       "    alpha     pred_col_name  \n",
       "21    0.1    CarbonEmission  \n",
       "20    0.1    CarbonEmission  \n",
       "23    0.1              WIND  \n",
       "22    0.1              WIND  \n",
       "12    0.1             Price  \n",
       "19    0.1              WIND  \n",
       "5     0.1             price  \n",
       "16    0.1             Price  \n",
       "8     0.1              MEDV  \n",
       "18    0.1               Age  \n",
       "2     0.1              Wage  \n",
       "3     0.1       childHeight  \n",
       "0     0.1              Fare  \n",
       "9     0.1            drinks  \n",
       "10    0.1         company10  \n",
       "11    0.1  Concretestrength  \n",
       "6     0.1           frailty  \n",
       "14    0.1     Selling_Price  \n",
       "13    0.1    traffic_volume  \n",
       "15    0.1            weight  \n",
       "17    0.1          Rainfall  \n",
       "7     0.1             Price  \n",
       "1     0.1   medical charges  \n",
       "4     0.1           quality  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values(by='prop_normal', ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "td6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
